# Session 2026-01-03

Prompts:

- You are an expert Python engineer. Build a complete, runnable Python project that ingests Chicago crime data daily from the City of Chicago Data Portal (Socrata SODA API) and serves an interactive analytics UI using Dash + Plotly.

HIGH-LEVEL GOALS
1) Daily ingestion from Chicago Data Portal “Crimes - 2001 to Present” dataset via SODA/SoQL.
2) Store the dataset as PARTITIONED PARQUET on local disk (data lake style).
3) Provide an interactive Dash app (Plotly) to explore trends, distributions, and maps.
4) Make ingestion incremental, idempotent, and resilient to restarts.
5) Provide good developer ergonomics: Makefile (or scripts), .env support, logging, basic tests, and clear README.

CONSTRAINTS / DESIGN
- Use open-source only.
- Storage: partitioned parquet under ./data/lake/crimes/
  Partition columns: year=YYYY, month=MM (and optionally day=DD for fine-grain). Choose year/month by default.
- Maintain ingest state in ./data/state/ingest_state.json (or sqlite). Track watermark by “date” (max ingested date) and also track last successful run timestamp.
- Implement backfill window: each daily run re-pulls the last N days (default 14) to catch late corrections; deduplicate on primary key “id”.
- Provide a config system using environment variables + defaults:
  CHI_CRIME_DATASET_ID (default: the standard crimes dataset id)
  SODA_APP_TOKEN (optional)
  START_DATE (optional; if no state exists, default to last 365 days)
  BACKFILL_DAYS (default 14)
  PAGE_LIMIT (default 50000)
  DATA_DIR (default ./data)
- Use robust pagination: $limit + $offset. Keep each API request under limit. Retry with exponential backoff on transient errors (429/5xx).
- For efficient UI filtering and aggregation, query Parquet using DuckDB (read_parquet) and return Pandas DataFrames.
- The UI must not call Socrata live; it must query local Parquet only.
- Include lightweight “dimension tables” computed on the fly (primary_type list, districts, etc.) and cache them to improve responsiveness.

PROJECT STRUCTURE (create exactly this)
chicago-crime-dash/
  pyproject.toml
  README.md
  .env.example
  Makefile
  chicago_crime_dash/
    __init__.py
    config.py
    logging_config.py
    ingest/
      __init__.py
      soda_client.py
      ingest_crimes.py
      state.py
      parquet_writer.py
      schema.py
    analytics/
      __init__.py
      queries.py
      aggregations.py
      geo.py
    app/
      __init__.py
      layout.py
      callbacks.py
      components.py
      server.py
  scripts/
    ingest_daily.py
    run_app.py
  tests/
    test_state.py
    test_dedupe.py
    test_duckdb_queries.py

DEPENDENCIES
- Python 3.11+
- dash, plotly
- pandas
- duckdb
- pyarrow
- requests
- python-dotenv
- tenacity (or custom retry)
- pytest

INGESTION DETAILS
- Socrata API base URL: https://data.cityofchicago.org/resource/{dataset_id}.json
- Query fields minimally needed for analytics and mapping:
  id, date, primary_type, description, location_description, arrest, domestic,
  beat, district, ward, community_area,
  latitude, longitude,
  iucr, fbi_code
- Parse date as timezone-aware (assume UTC or parse as given; store as timestamp).
- Normalize column names to snake_case.
- Deduplication:
  - The dataset has an “id” field. Ensure only one record per id in the lake.
  - Since we’re writing partitioned parquet, implement a small “merge/dedupe” strategy:
    * Write raw pull for the run to a staging parquet file
    * Then compact/merge partitions affected by the run:
      - For each (year, month) touched, read existing parquet files for that partition + staging rows for that partition into DuckDB, dedupe by id keeping max(date) (or last seen), and rewrite that partition to new parquet files (atomic swap).
    * Keep it correct before optimizing for extreme performance.
- Partitioning:
  - Add year = date.year, month = date.month to dataframe before writing.
  - Partition path: data/lake/crimes/year=YYYY/month=MM/*.parquet
- State:
  - ingest_state.json includes:
    {
      "dataset_id": "...",
      "watermark_max_date": "YYYY-MM-DDTHH:MM:SSZ",
      "last_run_at": "...",
      "backfill_days": 14,
      "rows_last_run": 12345
    }
- CLI:
  - `python -m chicago_crime_dash.ingest.ingest_crimes --once`
  - `python scripts/ingest_daily.py` (loads env, runs once)
- Provide example cron line in README:
  15 3 * * * cd /path/to/repo && . .venv/bin/activate && python scripts/ingest_daily.py >> data/logs/ingest.log 2>&1

DASH APP REQUIREMENTS
- Run with `python scripts/run_app.py`
- Provide a layout with:
  Left panel filters:
    - date range picker (default last 30 days available in local data)
    - dropdown multi-select primary_type
    - dropdown district (optional)
    - checklist arrest/domestic (optional)
    - slider for hour-of-day (optional) or separate control
  Main panels (at least these 5 charts):
    1) Time series: incidents per day or week (line)
    2) Top N bar: primary_type counts (bar)
    3) Heatmap: day-of-week vs hour-of-day counts
    4) Map: scatter_mapbox using latitude/longitude (only show points when date range <= 90 days; otherwise show aggregated hex/bin or warn and fallback to choropleth-like aggregation by community_area)
    5) Arrest rate by primary_type (bar with %)
- Use Plotly built-in map styles; do not require a paid Mapbox token. If token missing, still render using open style.
- Performance:
  - Use server-side filtering in DuckDB based on selected filters (date range + types + district).
  - Cache results for identical filter combinations using a simple LRU in memory.
- Add “Download CSV of filtered data” button (Dash dcc.Download).
- Add a small “Data freshness” indicator reading ingest_state.json (last_run_at).

ANALYTICS / QUERY LAYER
- Implement chicago_crime_dash/analytics/queries.py:
  - get_available_date_range()
  - filter_crimes(date_start, date_end, primary_types, district, arrest, domestic) -> DataFrame (bounded row count if needed)
  - time_series_counts(grain=day|week)
  - top_n_primary_types(n=15)
  - dow_hour_heatmap()
  - arrest_rate_by_type()
- Use DuckDB SQL against Parquet:
  SELECT ... FROM read_parquet('data/lake/crimes/**/*.parquet') WHERE ...
- Ensure correct handling when no data exists (friendly messages).

TESTS
- test_state.py: state read/write roundtrip and default init.
- test_dedupe.py: given a small existing partition + new staging rows with duplicate ids, ensure output contains unique ids.
- test_duckdb_queries.py: using a tiny local parquet in a temp directory, ensure filters and aggregations run.

README
Include:
- Setup instructions (venv, pip install -e .)
- How to run ingest once
- How to run app
- Cron example
- Notes on rate limits and optional SODA_APP_TOKEN
- Expected local disk layout

IMPLEMENTATION QUALITY BAR
- Code must run.
- Use type hints.
- Use logging with a consistent format.
- Handle errors gracefully and print actionable messages.
- Keep functions small and organized.
- Do not leave TODOs; finish the MVP end-to-end.

Now implement the full repository accordingly. Produce all files with correct imports and runnable entry points.

