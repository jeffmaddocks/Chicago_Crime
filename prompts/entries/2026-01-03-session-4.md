You are GitHub Copilot (Codex) operating on THIS repo. Read the current tree and modify files in-place. Follow existing conventions:

- venv workflow is driven by Makefile targets using .venv/bin/python
- configuration comes from chicago_crime/config.py via get_settings() and env vars in .env / .env.example
- ingest modules already exist under chicago_crime/ingest (ingest_crimes.py, ingest_dimensions.py, ingest_acs.py)
- there are helper scripts in /scripts (ingest_daily.py, run_app.py, startup.sh, shutdown.sh)
- the current docker-compose.yml includes chicago_crime_app and chicago_crime_ingest (profiles: ["ingest"])
- Dash/Plotly app currently runs via scripts/run_app.py and Dockerfile (CMD python scripts/run_app.py)
- The lake is in DATA_DIR (default ./data), and Parquet lives under settings.lake_dir (see config usage in analytics/queries.py)
- Tests exist under /tests and use monkeypatch to set DATA_DIR and reset config._SETTINGS

OBJECTIVE
---------
Move ALL ingest to venv (host) and keep Docker ONLY for Apache Superset.
Superset must read the same local ./data lake via a read-only mount.

Deliverables:
1) Remove/disable Docker-based ingest (no ingest container required).
2) Add docker-compose.superset.yml + a small custom Superset image that includes DuckDB SQLAlchemy driver.
3) Add a venv-first “DuckDB bridge” that creates ./data/lake/chicago_crime.duckdb with stable views referencing Parquet + dims (optional dims if present).
4) Update Makefile + README + scripts to reflect the new workflow, preserving existing style.

IMPORTANT: Keep Dash as optional/legacy. Do not break existing Dash workflow, but make Superset the recommended BI path.

========================================
A) REMOVE DOCKER-BASED INGEST (TAILORED)
========================================
1) Edit docker-compose.yml:
   - Remove the entire chicago_crime_ingest service.
   - Keep chicago_crime_app service unchanged (so legacy Dash Docker still works).
   - The file currently contains a literal line with “...” (invalid YAML). Remove any placeholder “...” lines so docker compose remains valid.

2) scripts/startup.sh currently assumes Docker first (it checks docker, calls docker compose, etc.) and also contains “...” placeholders.
   Replace it with a venv-first startup that:
   - DOES NOT require Docker to run ingest.
   - Does:
       * verify .env exists (like current script does)
       * verify .venv exists, and if not, print a clear message to run `make install`
       * run `make ingest` (host venv ingest, already exists)
       * run `make dims` (you will add this target; it runs scripts/ingest_daily.py OR individual dim ensures)
       * run `make duckdb` (you will add this target; it builds ./data/lake/chicago_crime.duckdb)
   - Then print next steps:
       * “Superset: make superset-up” and URL http://localhost:8088
       * “Legacy Dash: make app” OR “docker compose up chicago_crime_app”

3) scripts/shutdown.sh can stay Docker-focused, but update it to:
   - shut down ONLY legacy Dash compose (docker compose down) if present
   - shut down Superset via `docker compose -f docker-compose.superset.yml down` (new)
   - be safe if either is not running

========================================
B) ADD DOCKER-ONLY SUPERSET (TAILORED)
========================================
Add a new compose file at repo root: docker-compose.superset.yml (do NOT replace existing docker-compose.yml).

Use this layout (minimal changes allowed if needed for repo consistency):

version: "3.8"
services:
  superset:
    build:
      context: .
      dockerfile: superset/Dockerfile
    container_name: superset
    env_file:
      - .env.superset
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
      - ./data:/data:ro
      - ./superset/superset-init.sh:/app/superset-init.sh:ro
    depends_on:
      - superset_db
      - superset_redis
    command: ["/bin/bash", "-lc", "/app/superset-init.sh && superset run -h 0.0.0.0 -p 8088"]

  superset_db:
    image: postgres:15
    container_name: superset_db
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
    volumes:
      - superset_db:/var/lib/postgresql/data

  superset_redis:
    image: redis:7
    container_name: superset_redis

volumes:
  superset_home:
  superset_db:

Create /superset directory with:
1) superset/Dockerfile
   - FROM apache/superset:latest
   - Install DuckDB driver for SQLAlchemy:
       pip install --no-cache-dir duckdb duckdb-engine pyarrow
   - Keep it lean.

2) superset/superset-init.sh
   - Bash script, idempotent:
     - waits for Postgres and Redis (use python -c socket checks or nc; pick something that exists in image)
     - runs: superset db upgrade
     - creates admin user if missing using env vars:
         ADMIN_USERNAME, ADMIN_PASSWORD, ADMIN_FIRSTNAME, ADMIN_LASTNAME, ADMIN_EMAIL
       (Use `superset fab create-admin` in a way that won’t error if user exists; if it errors, catch and continue.)
     - runs: superset init
   - MUST set SUPERSET_CONFIG_PATH if you add a config file (optional). Prefer no extra config unless required.

Add:
- .env.superset.example with sane defaults (similar style to .env.example)
- Add .env.superset to .gitignore (repo already has .gitignore; append)
- Keep the repo’s .env workflow intact (don’t rename .env.example).

Minimum .env.superset.example content:
SUPERSET_SECRET_KEY=change_me
SUPERSET_LOAD_EXAMPLES=no
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin
ADMIN_FIRSTNAME=Superset
ADMIN_LASTNAME=Admin
ADMIN_EMAIL=admin@example.com
SQLALCHEMY_DATABASE_URI=postgresql+psycopg2://superset:superset@superset_db:5432/superset
REDIS_HOST=superset_redis
REDIS_PORT=6379

========================================
C) DUCKDB BRIDGE (TAILORED TO EXISTING SETTINGS)
========================================
This repo already depends on duckdb and uses it in chicago_crime/analytics/queries.py to read Parquet via glob.
For Superset, implement a stable DuckDB database file with views so the Superset “Database connection” is simple.

Create new module:
- chicago_crime/ingest/build_duckdb.py

Follow repo CLI style used in ingest_crimes.py (argparse + main()).

CLI:
python -m chicago_crime.ingest.build_duckdb --rebuild

Implementation details (use existing config conventions):
- Use get_settings() to locate:
  * settings.data_dir
  * settings.lake_dir (already referenced in analytics/queries.py)
  * dim paths from settings if they exist (population_dim_path, acs_dim_path, community_area dim path)
    If settings doesn’t expose some paths, derive them from data_dir to match existing scripts:
      data_dir/dim/community_areas/community_areas.parquet
      data_dir/dim/population/community_area_population.parquet
      data_dir/dim/acs_demographics/acs_demographics.parquet
- Target DuckDB file:
  db_path = settings.lake_dir / "chicago_crime.duckdb"
  (This yields ./data/lake/chicago_crime.duckdb on host, /data/lake/chicago_crime.duckdb in container.)

Behavior:
- Ensure settings.lake_dir exists.
- If --rebuild:
  - delete existing db_path if present (unlink)
- Connect to duckdb.connect(str(db_path))
- Create OR REPLACE VIEW crimes AS
    SELECT * FROM read_parquet('<lake_glob>')
  where <lake_glob> matches current lake convention used in analytics/queries.py:
    str(settings.lake_dir / "**" / "*.parquet")

- Conditionally create views if files exist:
  community_areas: read_parquet(community_areas.parquet)
  population: read_parquet(population parquet)
  acs_demographics: read_parquet(acs parquet)

- Create OR REPLACE VIEW crimes_enriched AS
    SELECT c.*, ca.<name_field> AS community_area_name
           , p.population AS population
           , a.* (or selected columns)
    FROM crimes c
    LEFT JOIN community_areas ca ON TRY_CAST(c.community_area AS INTEGER) = TRY_CAST(ca.community_area AS INTEGER)
    LEFT JOIN population p ON TRY_CAST(c.community_area AS INTEGER) = TRY_CAST(p.community_area AS INTEGER)
    LEFT JOIN acs_demographics a ON TRY_CAST(c.community_area AS INTEGER) = TRY_CAST(a.community_area AS INTEGER)
  BUT:
    - Don’t assume column names blindly. Reuse the join-key conventions already tested:
      * community_area join key should be an int
    - Keep it robust:
      * If a dim file doesn’t exist, skip that join and still create crimes_enriched with what’s available.
      * If a dim exists but doesn’t have expected columns, log warning and continue (create a simpler enriched view).
- Log where the db was written.

Add a small unit test (fast, no network):
- tests/test_build_duckdb.py
- Use tmp_path, monkeypatch DATA_DIR, reset config._SETTINGS like other tests.
- Create a minimal parquet under tmp_path/data/lake/ (or settings.lake_dir) with a few crimes rows.
- Optionally create a dim parquet.
- Run build_duckdb.main() programmatically (or subprocess-free call) and assert:
  - db file exists
  - querying duckdb shows view “crimes” exists and returns rows
  - “crimes_enriched” exists

========================================
D) MAKEFILE + README (TAILORED)
========================================
Update Makefile (keep existing targets intact) and add:

dims:
	$(BIN)/python scripts/ingest_daily.py
  (This script already ensures community areas + population + acs + then calls ingest_main(). If you prefer not to re-run crimes on dims, then instead run:
    $(BIN)/python -m chicago_crime.ingest.ingest_dimensions
    $(BIN)/python -m chicago_crime.ingest.ingest_acs
   BUT prefer the existing convention: scripts/ingest_daily.py, unless it causes duplication. If it re-runs crimes, add a new script or option to only run dims; choose the cleanest approach with minimal code changes.)

duckdb:
	$(BIN)/python -m chicago_crime.ingest.build_duckdb --rebuild

superset-up:
	docker compose -f docker-compose.superset.yml up -d --build

superset-down:
	docker compose -f docker-compose.superset.yml down

superset-logs:
	docker compose -f docker-compose.superset.yml logs -f superset

Update README.md (preserve existing sections, but add a clear “Recommended: Superset” section near the top):
- Venv-first ingest:
  1) cp .env.example .env
  2) make install
  3) make ingest   (or make dims if that’s your daily workflow)
  4) make duckdb
- Superset (Docker-only):
  1) cp .env.superset.example .env.superset
  2) make superset-up
  3) open http://localhost:8088 (admin/admin by default)
  4) Add Database in Superset:
     - SQLAlchemy URI: duckdb:////data/lake/chicago_crime.duckdb
  5) Create datasets from views: crimes, crimes_enriched, community_areas, population, acs_demographics

Also note:
- Superset sees the repo lake at /data (read-only mount).
- DuckDB file is created by the host venv workflow (make duckdb).

========================================
E) CONSISTENCY CLEANUP (TAILORED)
========================================
This repo currently has some literal “...” placeholders in tracked files (docker-compose.yml, scripts/run_app.py, scripts/startup.sh, config.py, tests/test_duckdb_queries.py, etc.).
Do NOT attempt to “reconstruct” missing logic that is unknown.
Instead:
- remove placeholder “...” lines where they break execution (YAML / Python)
- if a file is truly truncated by placeholders in a way that makes it invalid Python, fix it by restoring minimal valid code consistent with existing patterns:
  * For scripts/run_app.py: ensure it imports, conditionally runs ensure_community_areas_dim(), and starts app_main().
  * For config.py: ensure Settings dataclass and get_settings() return a fully working object consistent with .env.example keys.
  * For tests: ensure they are valid python; if the placeholder is inside a test body, remove it and keep assertions.

However, prioritize the Superset + ingest separation work. Only fix placeholder issues that prevent running tests or the app.

========================================
ACCEPTANCE CRITERIA
========================================
- `make ingest` runs on host venv, no Docker required.
- `docker compose -f docker-compose.superset.yml up -d --build` starts Superset at :8088.
- Superset container has /data mounted read-only from repo ./data.
- Superset image can connect to DuckDB using `duckdb:////data/lake/chicago_crime.duckdb`.
- `make duckdb` creates ./data/lake/chicago_crime.duckdb and views exist.
- Tests include a new test for build_duckdb and the suite passes.

Now implement all changes. Output:
1) The full contents of each new/modified file.
2) A short bullet summary of what you changed and how to run the new workflow.